{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math, re, os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "GCS_PATH = \"input/cassava-leaf-disease-classification\"\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [512, 512]\n",
    "CLASSES = ['0', '1', '2', '3', '4']\n",
    "EPOCHS = 25\n",
    "\n",
    "\n",
    "SEED = 752\n",
    "SKIP_VALIDATION = False\n",
    "TTA_NUM = 5\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    } if labeled else {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    if labeled:\n",
    "        label = tf.cast(example['target'], tf.int32)\n",
    "        return image, label\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n",
    "    tf.io.gfile.glob(GCS_PATH + '/train_tfrecords/ld_train*.tfrec'),\n",
    "    test_size=0.35, random_state=5\n",
    ")\n",
    "\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test_tfrecords/ld_test*.tfrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_blockout(img, sl=0.1, sh=0.2, rl=0.4):\n",
    "    p=random.random()\n",
    "    if p>=0.25:\n",
    "        w, h, c = IMAGE_SIZE[0], IMAGE_SIZE[1], 3\n",
    "        origin_area = tf.cast(h*w, tf.float32)\n",
    "\n",
    "        e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n",
    "        e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n",
    "\n",
    "        e_height_h = tf.minimum(e_size_h, h)\n",
    "        e_width_h = tf.minimum(e_size_h, w)\n",
    "\n",
    "        erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n",
    "        erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n",
    "\n",
    "        erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n",
    "        erase_area = tf.cast(erase_area, tf.uint8)\n",
    "\n",
    "        pad_h = h - erase_height\n",
    "        pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n",
    "        pad_bottom = pad_h - pad_top\n",
    "\n",
    "        pad_w = w - erase_width\n",
    "        pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n",
    "        erase_mask = tf.squeeze(erase_mask, axis=0)\n",
    "        erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n",
    "\n",
    "        return tf.cast(erased_img, img.dtype)\n",
    "    else:\n",
    "        return tf.cast(img, img.dtype)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_augment(image, label):\n",
    "    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n",
    "    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = random_blockout(image)\n",
    "    return image, label\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
    "def transform(image,label):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = 15. * tf.random.normal([1],dtype='float32')\n",
    "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "  \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM,DIM,3]),label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(TRAINING_FILENAMES):\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)  \n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \n",
    "    dataset = dataset.map(transform, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "def get_validation_dataset(VALID_FILENAMES,ordered=False):\n",
    "    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "def get_test_dataset(TEST_FILENAMES,ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "\n",
    "print('Dataset: {} training images, {} validation images, {} (unlabeled) test images'.format(\n",
    "    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package_path = 'input/efficientnet/'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "package_path = 'input/kerasapplications'\n",
    "sys.path.append(package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "efficientnet_model = tf.keras.models.load_model('input/cassava-leaf-disease-efficient-and-dense-net/effcient_net.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model = tf.keras.models.load_model('input/cassava-leaf-disease-efficient-and-dense-net/dense_net.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_models = [tf.keras.models.load_model(f'input/cassava-leaf-disease-resnet50-model/resnet50/fold-{i}.h5') for i in range(4,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_index,model in enumerate(resnet50_models):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        model.layers[i]._name  = 'resnet50_'+str(model_index)+\"_\" + str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101_models = [tf.keras.models.load_model(f'input/cassava-leaf-disease-resnet101-model/resnet101/fold-{i}.h5') for i in range(4,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_index,model in enumerate(resnet101_models):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        model.layers[i]._name  = 'resnet101_'+str(model_index)+\"_\" + str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNext 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnext101_models = [tf.keras.models.load_model(f'input/cassava-leaf-disease-resnext101-model/resnext101/fold-{i}.h5') for i in range(4,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_index,model in enumerate(resnext101_models):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        model.layers[i]._name  = 'resnext101_'+str(model_index)+\"_\" + str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [efficientnet_model,densenet_model]\n",
    "# all_models.extend(resnet50_models)\n",
    "# all_models.extend(resnet101_models)\n",
    "all_models.extend(resnext101_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(all_models):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    input_ = tf.keras.layers.Input(shape=(512, 512, 3))\n",
    "    \n",
    "    ensemble_outputs = []\n",
    "    \n",
    "    for model in all_models:\n",
    "\n",
    "        ensemble_output = model(input_) \n",
    "        ensemble_outputs.append(ensemble_output)\n",
    "        \n",
    "    merge = tf.keras.layers.concatenate(ensemble_outputs)\n",
    "    \n",
    "    merge = tf.keras.layers.Dense(100, activation='relu',name=\"layer_dense_prelim\")(merge)\n",
    "    output = tf.keras.layers.Dense(len(CLASSES), activation='softmax',name=\"ensembled_output\")(merge)\n",
    "\n",
    "    ensembled_model = tf.keras.models.Model(inputs=input_, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi Tempered Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop(num_iters, body, initial_args):\n",
    "  \"\"\"Runs a simple for-loop with given body and initial_args.\n",
    "  Args:\n",
    "    num_iters: Maximum number of iterations.\n",
    "    body: Body of the for-loop.\n",
    "    initial_args: Args to the body for the first iteration.\n",
    "  Returns:\n",
    "    Output of the final iteration.\n",
    "  \"\"\"\n",
    "  for i in range(num_iters):\n",
    "    if i == 0:\n",
    "      outputs = body(*initial_args)\n",
    "    else:\n",
    "      outputs = body(*outputs)\n",
    "  return outputs\n",
    "\n",
    "\n",
    "def log_t(u, t):\n",
    "  \"\"\"Compute log_t for `u`.\"\"\"\n",
    "\n",
    "  def _internal_log_t(u, t):\n",
    "    return (u**(1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "  return tf.cond(\n",
    "      tf.equal(t, 1.0), lambda: tf.log(u),\n",
    "      functools.partial(_internal_log_t, u, t))\n",
    "\n",
    "\n",
    "def exp_t(u, t):\n",
    "  \"\"\"Compute exp_t for `u`.\"\"\"\n",
    "\n",
    "  def _internal_exp_t(u, t):\n",
    "    return tf.nn.relu(1.0 + (1.0 - t) * u)**(1.0 / (1.0 - t))\n",
    "\n",
    "  return tf.cond(\n",
    "      tf.equal(t, 1.0), lambda: tf.exp(u),\n",
    "      functools.partial(_internal_exp_t, u, t))\n",
    "\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters=5):\n",
    "  \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "  \"\"\"\n",
    "\n",
    "  mu = tf.reduce_max(activations, -1, keep_dims=True)\n",
    "  normalized_activations_step_0 = activations - mu\n",
    "  shape_normalized_activations = tf.shape(normalized_activations_step_0)\n",
    "\n",
    "  def iter_body(i, normalized_activations):\n",
    "    logt_partition = tf.reduce_sum(\n",
    "        exp_t(normalized_activations, t), -1, keep_dims=True)\n",
    "    normalized_activations_t = tf.reshape(\n",
    "        normalized_activations_step_0 * tf.pow(logt_partition, 1.0 - t),\n",
    "        shape_normalized_activations)\n",
    "    return [i + 1, normalized_activations_t]\n",
    "\n",
    "  _, normalized_activations_t = for_loop(num_iters, iter_body,\n",
    "                                         [0, normalized_activations_step_0])\n",
    "  logt_partition = tf.reduce_sum(\n",
    "      exp_t(normalized_activations_t, t), -1, keep_dims=True)\n",
    "  return -log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "\n",
    "def compute_normalization_binary_search(activations, t, num_iters=10):\n",
    "  \"\"\"Returns the normalization value for each example (t < 1.0).\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (< 1.0 for finite support).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "  \"\"\"\n",
    "  mu = tf.reduce_max(activations, -1, keep_dims=True)\n",
    "  normalized_activations = activations - mu\n",
    "  shape_activations = tf.shape(activations)\n",
    "  effective_dim = tf.cast(\n",
    "      tf.reduce_sum(\n",
    "          tf.cast(\n",
    "              tf.greater(normalized_activations, -1.0 / (1.0 - t)), tf.int32),\n",
    "          -1,\n",
    "          keep_dims=True), tf.float32)\n",
    "  shape_partition = tf.concat([shape_activations[:-1], [1]], 0)\n",
    "  lower = tf.zeros(shape_partition)\n",
    "  upper = -log_t(1.0 / effective_dim, t) * tf.ones(shape_partition)\n",
    "\n",
    "  def iter_body(i, lower, upper):\n",
    "    logt_partition = (upper + lower)/2.0\n",
    "    sum_probs = tf.reduce_sum(exp_t(\n",
    "        normalized_activations - logt_partition, t), -1, keep_dims=True)\n",
    "    update = tf.cast(tf.less(sum_probs, 1.0), tf.float32)\n",
    "    lower = tf.reshape(lower * update + (1.0 - update) * logt_partition,\n",
    "                       shape_partition)\n",
    "    upper = tf.reshape(upper * (1.0 - update) + update * logt_partition,\n",
    "                       shape_partition)\n",
    "    return [i + 1, lower, upper]\n",
    "\n",
    "  _, lower, upper = for_loop(num_iters, iter_body, [0, lower, upper])\n",
    "  logt_partition = (upper + lower)/2.0\n",
    "  return logt_partition + mu\n",
    "\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "  \"\"\"Returns the normalization value for each example.\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "  \"\"\"\n",
    "  return tf.cond(\n",
    "      tf.less(t, 1.0),\n",
    "      functools.partial(compute_normalization_binary_search, activations, t,\n",
    "                        num_iters),\n",
    "      functools.partial(compute_normalization_fixed_point, activations, t,\n",
    "                        num_iters))\n",
    "\n",
    "\n",
    "def _internal_bi_tempered_logistic_loss(activations, labels, t1, t2):\n",
    "  \"\"\"Computes the Bi-Tempered logistic loss.\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    labels: batch_size\n",
    "    t1: Temperature 1 (< 1.0 for boundedness).\n",
    "    t2: Temperature 2 (> 1.0 for tail heaviness).\n",
    "  Returns:\n",
    "    A loss tensor for robust loss.\n",
    "  \"\"\"\n",
    "  if t2 == 1.0:\n",
    "    normalization_constants = tf.log(\n",
    "        tf.reduce_sum(tf.exp(activations), -1, keep_dims=True))\n",
    "    if t1 == 1.0:\n",
    "      return normalization_constants + tf.reduce_sum(\n",
    "          tf.multiply(labels, tf.log(labels + 1e-10) - activations), -1)\n",
    "    else:\n",
    "      shifted_activations = tf.exp(activations - normalization_constants)\n",
    "      one_minus_t1 = (1.0 - t1)\n",
    "      one_minus_t2 = 1.0\n",
    "  else:\n",
    "    one_minus_t1 = (1.0 - t1)\n",
    "    one_minus_t2 = (1.0 - t2)\n",
    "    normalization_constants = compute_normalization(\n",
    "        activations, t2, num_iters=5)\n",
    "    shifted_activations = tf.nn.relu(1.0 + one_minus_t2 *\n",
    "                                     (activations - normalization_constants))\n",
    "\n",
    "  if t1 == 1.0:\n",
    "    return tf.reduce_sum(\n",
    "        tf.multiply(\n",
    "            tf.log(labels + 1e-10) -\n",
    "            tf.log(tf.pow(shifted_activations, 1.0 / one_minus_t2)), labels),\n",
    "        -1)\n",
    "  else:\n",
    "    beta = 1.0 + one_minus_t1\n",
    "    logt_probs = (tf.pow(shifted_activations, one_minus_t1 / one_minus_t2) -\n",
    "                  1.0) / one_minus_t1\n",
    "    return tf.reduce_sum(\n",
    "        tf.multiply(log_t(labels, t1) - logt_probs, labels) - 1.0 / beta *\n",
    "        (tf.pow(labels, beta) -\n",
    "         tf.pow(shifted_activations, beta / one_minus_t2)), -1)\n",
    "\n",
    "\n",
    "def tempered_sigmoid(activations, t, num_iters=5):\n",
    "  \"\"\"Tempered sigmoid function.\n",
    "  Args:\n",
    "    activations: Activations for the positive class for binary classification.\n",
    "    t: Temperature tensor > 0.0.\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Returns:\n",
    "    A probabilities tensor.\n",
    "  \"\"\"\n",
    "  t = tf.convert_to_tensor(t)\n",
    "  input_shape = tf.shape(activations)\n",
    "  activations_2d = tf.reshape(activations, [-1, 1])\n",
    "  internal_activations = tf.concat(\n",
    "      [tf.zeros_like(activations_2d), activations_2d], 1)\n",
    "  normalization_constants = tf.cond(\n",
    "      # pylint: disable=g-long-lambda\n",
    "      tf.equal(t, 1.0),\n",
    "      lambda: tf.log(\n",
    "          tf.reduce_sum(tf.exp(internal_activations), -1, keep_dims=True)),\n",
    "      functools.partial(compute_normalization, internal_activations, t,\n",
    "                        num_iters))\n",
    "  internal_probabilities = exp_t(internal_activations - normalization_constants,\n",
    "                                 t)\n",
    "  one_class_probabilities = tf.split(internal_probabilities, 2, axis=1)[1]\n",
    "  return tf.reshape(one_class_probabilities, input_shape)\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "  \"\"\"Tempered softmax function.\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature tensor > 0.0.\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Returns:\n",
    "    A probabilities tensor.\n",
    "  \"\"\"\n",
    "  t = tf.convert_to_tensor(t)\n",
    "  normalization_constants = tf.cond(\n",
    "      tf.equal(t, 1.0),\n",
    "      lambda: tf.log(tf.reduce_sum(tf.exp(activations), -1, keep_dims=True)),\n",
    "      functools.partial(compute_normalization, activations, t, num_iters))\n",
    "  return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "\n",
    "def bi_tempered_binary_logistic_loss(activations,\n",
    "                                     labels,\n",
    "                                     t1,\n",
    "                                     t2,\n",
    "                                     label_smoothing=0.0,\n",
    "                                     num_iters=5):\n",
    "  \"\"\"Bi-Tempered binary logistic loss.\n",
    "  Args:\n",
    "    activations: A tensor containing activations for class 1.\n",
    "    labels: A tensor with shape and dtype as activations.\n",
    "    t1: Temperature 1 (< 1.0 for boundedness).\n",
    "    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "    label_smoothing: Label smoothing\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Returns:\n",
    "    A loss tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('binary_bitempered_logistic'):\n",
    "    t1 = tf.convert_to_tensor(t1)\n",
    "    t2 = tf.convert_to_tensor(t2)\n",
    "    out_shape = tf.shape(labels)\n",
    "    labels_2d = tf.reshape(labels, [-1, 1])\n",
    "    activations_2d = tf.reshape(activations, [-1, 1])\n",
    "    internal_labels = tf.concat([1.0 - labels_2d, labels_2d], 1)\n",
    "    internal_logits = tf.concat([tf.zeros_like(activations_2d), activations_2d],\n",
    "                                1)\n",
    "    losses = bi_tempered_logistic_loss(internal_logits, internal_labels, t1, t2,\n",
    "                                       label_smoothing, num_iters)\n",
    "    return tf.reshape(losses, out_shape)\n",
    "\n",
    "\n",
    "def bi_tempered_logistic_loss(activations,\n",
    "                              labels,\n",
    "                              t1,\n",
    "                              t2,\n",
    "                              label_smoothing=0.0,\n",
    "                              num_iters=5):\n",
    "  \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    labels: A tensor with shape and dtype as activations.\n",
    "    t1: Temperature 1 (< 1.0 for boundedness).\n",
    "    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "    label_smoothing: Label smoothing parameter between [0, 1).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Returns:\n",
    "    A loss tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('bitempered_logistic'):\n",
    "    t1 = tf.convert_to_tensor(t1)\n",
    "    t2 = tf.convert_to_tensor(t2)\n",
    "    if label_smoothing > 0.0:\n",
    "      num_classes = tf.cast(tf.shape(labels)[-1], tf.float32)\n",
    "      labels = (\n",
    "          1 - num_classes /\n",
    "          (num_classes - 1) * label_smoothing) * labels + label_smoothing / (\n",
    "              num_classes - 1)\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def _custom_gradient_bi_tempered_logistic_loss(activations):\n",
    "      \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n",
    "      Args:\n",
    "        activations: A multi-dimensional tensor with last dim `num_classes`.\n",
    "      Returns:\n",
    "        A loss tensor, grad.\n",
    "      \"\"\"\n",
    "      with tf.name_scope('gradient_bitempered_logistic'):\n",
    "        probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "        loss_values = tf.multiply(\n",
    "            labels,\n",
    "            log_t(labels + 1e-10, t1) -\n",
    "            log_t(probabilities, t1)) - 1.0 / (2.0 - t1) * (\n",
    "                tf.pow(labels, 2.0 - t1) - tf.pow(probabilities, 2.0 - t1))\n",
    "\n",
    "        def grad(d_loss):\n",
    "          \"\"\"Explicit gradient calculation.\n",
    "          Args:\n",
    "            d_loss: Infinitesimal change in the loss value.\n",
    "          Returns:\n",
    "            Loss gradient.\n",
    "          \"\"\"\n",
    "          delta_probs = probabilities - labels\n",
    "          forget_factor = tf.pow(probabilities, t2 - t1)\n",
    "          delta_probs_times_forget_factor = tf.multiply(delta_probs,\n",
    "                                                        forget_factor)\n",
    "          delta_forget_sum = tf.reduce_sum(\n",
    "              delta_probs_times_forget_factor, -1, keep_dims=True)\n",
    "          escorts = tf.pow(probabilities, t2)\n",
    "          escorts = escorts / tf.reduce_sum(escorts, -1, keep_dims=True)\n",
    "          derivative = delta_probs_times_forget_factor - tf.multiply(\n",
    "              escorts, delta_forget_sum)\n",
    "          return tf.multiply(d_loss, derivative)\n",
    "\n",
    "        return loss_values, grad\n",
    "\n",
    "    loss_values = tf.cond(tf.logical_and(tf.equal(t1, 1.0), tf.equal(t2, 1.0)),\n",
    "                          functools.partial(\n",
    "                              tf.nn.softmax_cross_entropy_with_logits,\n",
    "                              labels=labels,\n",
    "                              logits=activations),\n",
    "                          functools.partial(\n",
    "                              _custom_gradient_bi_tempered_logistic_loss,\n",
    "                              activations))\n",
    "    reduce_sum_last = lambda x: tf.reduce_sum(x, -1)\n",
    "    loss_values = tf.cond(tf.logical_and(tf.equal(t1, 1.0), tf.equal(t2, 1.0)),\n",
    "                          functools.partial(tf.identity, loss_values),\n",
    "                          functools.partial(reduce_sum_last, loss_values))\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def sparse_bi_tempered_logistic_loss(activations, labels, t1, t2, num_iters=5):\n",
    "  \"\"\"Sparse Bi-Tempered Logistic Loss with custom gradient.\n",
    "  Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    labels: A tensor with dtype of int32.\n",
    "    t1: Temperature 1 (< 1.0 for boundedness).\n",
    "    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "  Returns:\n",
    "    A loss tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('sparse_bitempered_logistic'):\n",
    "    t1 = tf.convert_to_tensor(t1)\n",
    "    t2 = tf.convert_to_tensor(t2)\n",
    "    num_classes = tf.shape(activations)[-1]\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def _custom_gradient_sparse_bi_tempered_logistic_loss(activations):\n",
    "      \"\"\"Sparse Bi-Tempered Logistic Loss with custom gradient.\n",
    "      Args:\n",
    "        activations: A multi-dimensional tensor with last dim `num_classes`.\n",
    "      Returns:\n",
    "        A loss tensor, grad.\n",
    "      \"\"\"\n",
    "      with tf.name_scope('gradient_sparse_bitempered_logistic'):\n",
    "        probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "        # TODO(eamid): Replace one hot with gather.\n",
    "        loss_values = -log_t(\n",
    "            tf.reshape(\n",
    "                tf.gather_nd(probabilities,\n",
    "                             tf.where(tf.one_hot(labels, num_classes))),\n",
    "                tf.shape(activations)[:-1]), t1) - 1.0 / (2.0 - t1) * (\n",
    "                    1.0 - tf.reduce_sum(tf.pow(probabilities, 2.0 - t1), -1))\n",
    "\n",
    "        def grad(d_loss):\n",
    "          \"\"\"Explicit gradient calculation.\n",
    "          Args:\n",
    "            d_loss: Infinitesimal change in the loss value.\n",
    "          Returns:\n",
    "            Loss gradient.\n",
    "          \"\"\"\n",
    "          delta_probs = probabilities - tf.one_hot(labels, num_classes)\n",
    "          forget_factor = tf.pow(probabilities, t2 - t1)\n",
    "          delta_probs_times_forget_factor = tf.multiply(delta_probs,\n",
    "                                                        forget_factor)\n",
    "          delta_forget_sum = tf.reduce_sum(\n",
    "              delta_probs_times_forget_factor, -1, keep_dims=True)\n",
    "          escorts = tf.pow(probabilities, t2)\n",
    "          escorts = escorts / tf.reduce_sum(escorts, -1, keep_dims=True)\n",
    "          derivative = delta_probs_times_forget_factor - tf.multiply(\n",
    "              escorts, delta_forget_sum)\n",
    "          return tf.multiply(d_loss, derivative)\n",
    "\n",
    "        return loss_values, grad\n",
    "\n",
    "    loss_values = tf.cond(\n",
    "        tf.logical_and(tf.equal(t1, 1.0), tf.equal(t2, 1.0)),\n",
    "        functools.partial(tf.nn.sparse_softmax_cross_entropy_with_logits,\n",
    "                          labels=labels, logits=activations),\n",
    "        functools.partial(_custom_gradient_sparse_bi_tempered_logistic_loss,\n",
    "                          activations))\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.001), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(TRAINING_FILENAMES)\n",
    "val_dataset =get_validation_dataset(VALID_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping( patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ensembled_model.fit(train_dataset,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=VALID_STEPS,callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_model.save(\"output/ensembled_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_test_dataset(TEST_FILENAMES)\n",
    "test_images_ds = test_ds.map(lambda image, idnum: image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = ensembled_model.predict(test_images_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating predictions...')\n",
    "\n",
    "predictions = np.argmax(ensemble_predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating submission file...')\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='image_id,label', comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
