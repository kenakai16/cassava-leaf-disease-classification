{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport math, re, os, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\n\n\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\nprint(\"Tensorflow version \" + tf.__version__)\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = \"../input/cassava-leaf-disease-classification\"\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [512, 512]\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 25\n\n\nSEED = 752\nSKIP_VALIDATION = False\nTTA_NUM = 5\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '/train_tfrecords/ld_train*.tfrec'),\n    test_size=0.35, random_state=5\n)\n\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test_tfrecords/ld_test*.tfrec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_blockout(img, sl=0.1, sh=0.2, rl=0.4):\n    p=random.random()\n    if p>=0.25:\n        w, h, c = IMAGE_SIZE[0], IMAGE_SIZE[1], 3\n        origin_area = tf.cast(h*w, tf.float32)\n\n        e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n        e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n\n        e_height_h = tf.minimum(e_size_h, h)\n        e_width_h = tf.minimum(e_size_h, w)\n\n        erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n        erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n        erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n        erase_area = tf.cast(erase_area, tf.uint8)\n\n        pad_h = h - erase_height\n        pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n        pad_bottom = pad_h - pad_top\n\n        pad_w = w - erase_width\n        pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n        pad_right = pad_w - pad_left\n\n        erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n        erase_mask = tf.squeeze(erase_mask, axis=0)\n        erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n        return tf.cast(erased_img, img.dtype)\n    else:\n        return tf.cast(img, img.dtype)\n\n\n\n\ndef data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    image = random_blockout(image)\n    return image, label\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\ndef transform(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_dataset(TRAINING_FILENAMES):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)  \n    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.map(transform, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\ndef get_validation_dataset(VALID_FILENAMES,ordered=False):\n    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\ndef get_test_dataset(TEST_FILENAMES,ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n\nprint('Dataset: {} training images, {} validation images, {} (unlabeled) test images'.format(\n    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Efficient Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\npackage_path = '../input/efficientnet/'\nsys.path.append(package_path)\n\npackage_path = '../input/kerasapplications'\nsys.path.append(package_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import efficientnet.tfkeras\n# from tensorflow.keras.models import load_model\n\nefficientnet_model = tf.keras.models.load_model('../input/cassava-leaf-disease-training/effcient_net.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dense Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"densenet_model = tf.keras.models.load_model('../input/cassava-leaf-disease-training/dense_net.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resnet50"},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet50_models = [tf.keras.models.load_model(f'../input/cassava-leaf-disease-resnet50/resnet50/fold-{i}.h5') for i in range(4,5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_index,model in enumerate(resnet50_models):\n    for i, layer in enumerate(model.layers):\n        model.layers[i]._name  = 'resnet50_'+str(model_index)+\"_\" + str(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resnet 101"},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet101_models = [tf.keras.models.load_model(f'../input/cassava-leaf-disease-resnet101/resnet101/fold-{i}.h5') for i in range(4,5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_index,model in enumerate(resnet101_models):\n    for i, layer in enumerate(model.layers):\n        model.layers[i]._name  = 'resnet101_'+str(model_index)+\"_\" + str(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNext 101"},{"metadata":{"trusted":true},"cell_type":"code","source":"resnext101_models = [tf.keras.models.load_model(f'../input/cassava-leaf-disease-resnext101/resnext101/fold-{i}.h5') for i in range(4,5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_index,model in enumerate(resnext101_models):\n    for i, layer in enumerate(model.layers):\n        model.layers[i]._name  = 'resnext101_'+str(model_index)+\"_\" + str(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [efficientnet_model,densenet_model]\n# all_models.extend(resnet50_models)\n# all_models.extend(resnet101_models)\nall_models.extend(resnext101_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Freeze layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(all_models):\n    for layer in model.layers:\n        layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensembled Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    input_ = tf.keras.layers.Input(shape=(512, 512, 3))\n    \n    ensemble_outputs = []\n    \n    for model in all_models:\n\n        ensemble_output = model(input_) \n        ensemble_outputs.append(ensemble_output)\n        \n    merge = tf.keras.layers.concatenate(ensemble_outputs)\n    \n    merge = tf.keras.layers.Dense(100, activation='relu',name=\"layer_dense_prelim\")(merge)\n    output = tf.keras.layers.Dense(len(CLASSES), activation='softmax',name=\"ensembled_output\")(merge)\n\n    ensembled_model = tf.keras.models.Model(inputs=input_, outputs=output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bi Tempered Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"def for_loop(num_iters, body, initial_args):\n  \"\"\"Runs a simple for-loop with given body and initial_args.\n  Args:\n    num_iters: Maximum number of iterations.\n    body: Body of the for-loop.\n    initial_args: Args to the body for the first iteration.\n  Returns:\n    Output of the final iteration.\n  \"\"\"\n  for i in range(num_iters):\n    if i == 0:\n      outputs = body(*initial_args)\n    else:\n      outputs = body(*outputs)\n  return outputs\n\n\ndef log_t(u, t):\n  \"\"\"Compute log_t for `u`.\"\"\"\n\n  def _internal_log_t(u, t):\n    return (u**(1.0 - t) - 1.0) / (1.0 - t)\n\n  return tf.cond(\n      tf.equal(t, 1.0), lambda: tf.log(u),\n      functools.partial(_internal_log_t, u, t))\n\n\ndef exp_t(u, t):\n  \"\"\"Compute exp_t for `u`.\"\"\"\n\n  def _internal_exp_t(u, t):\n    return tf.nn.relu(1.0 + (1.0 - t) * u)**(1.0 / (1.0 - t))\n\n  return tf.cond(\n      tf.equal(t, 1.0), lambda: tf.exp(u),\n      functools.partial(_internal_exp_t, u, t))\n\n\ndef compute_normalization_fixed_point(activations, t, num_iters=5):\n  \"\"\"Returns the normalization value for each example (t > 1.0).\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (> 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n  Return: A tensor of same rank as activation with the last dimension being 1.\n  \"\"\"\n\n  mu = tf.reduce_max(activations, -1, keep_dims=True)\n  normalized_activations_step_0 = activations - mu\n  shape_normalized_activations = tf.shape(normalized_activations_step_0)\n\n  def iter_body(i, normalized_activations):\n    logt_partition = tf.reduce_sum(\n        exp_t(normalized_activations, t), -1, keep_dims=True)\n    normalized_activations_t = tf.reshape(\n        normalized_activations_step_0 * tf.pow(logt_partition, 1.0 - t),\n        shape_normalized_activations)\n    return [i + 1, normalized_activations_t]\n\n  _, normalized_activations_t = for_loop(num_iters, iter_body,\n                                         [0, normalized_activations_step_0])\n  logt_partition = tf.reduce_sum(\n      exp_t(normalized_activations_t, t), -1, keep_dims=True)\n  return -log_t(1.0 / logt_partition, t) + mu\n\n\ndef compute_normalization_binary_search(activations, t, num_iters=10):\n  \"\"\"Returns the normalization value for each example (t < 1.0).\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (< 1.0 for finite support).\n    num_iters: Number of iterations to run the method.\n  Return: A tensor of same rank as activation with the last dimension being 1.\n  \"\"\"\n  mu = tf.reduce_max(activations, -1, keep_dims=True)\n  normalized_activations = activations - mu\n  shape_activations = tf.shape(activations)\n  effective_dim = tf.cast(\n      tf.reduce_sum(\n          tf.cast(\n              tf.greater(normalized_activations, -1.0 / (1.0 - t)), tf.int32),\n          -1,\n          keep_dims=True), tf.float32)\n  shape_partition = tf.concat([shape_activations[:-1], [1]], 0)\n  lower = tf.zeros(shape_partition)\n  upper = -log_t(1.0 / effective_dim, t) * tf.ones(shape_partition)\n\n  def iter_body(i, lower, upper):\n    logt_partition = (upper + lower)/2.0\n    sum_probs = tf.reduce_sum(exp_t(\n        normalized_activations - logt_partition, t), -1, keep_dims=True)\n    update = tf.cast(tf.less(sum_probs, 1.0), tf.float32)\n    lower = tf.reshape(lower * update + (1.0 - update) * logt_partition,\n                       shape_partition)\n    upper = tf.reshape(upper * (1.0 - update) + update * logt_partition,\n                       shape_partition)\n    return [i + 1, lower, upper]\n\n  _, lower, upper = for_loop(num_iters, iter_body, [0, lower, upper])\n  logt_partition = (upper + lower)/2.0\n  return logt_partition + mu\n\n\ndef compute_normalization(activations, t, num_iters=5):\n  \"\"\"Returns the normalization value for each example.\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n  Return: A tensor of same rank as activation with the last dimension being 1.\n  \"\"\"\n  return tf.cond(\n      tf.less(t, 1.0),\n      functools.partial(compute_normalization_binary_search, activations, t,\n                        num_iters),\n      functools.partial(compute_normalization_fixed_point, activations, t,\n                        num_iters))\n\n\ndef _internal_bi_tempered_logistic_loss(activations, labels, t1, t2):\n  \"\"\"Computes the Bi-Tempered logistic loss.\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    labels: batch_size\n    t1: Temperature 1 (< 1.0 for boundedness).\n    t2: Temperature 2 (> 1.0 for tail heaviness).\n  Returns:\n    A loss tensor for robust loss.\n  \"\"\"\n  if t2 == 1.0:\n    normalization_constants = tf.log(\n        tf.reduce_sum(tf.exp(activations), -1, keep_dims=True))\n    if t1 == 1.0:\n      return normalization_constants + tf.reduce_sum(\n          tf.multiply(labels, tf.log(labels + 1e-10) - activations), -1)\n    else:\n      shifted_activations = tf.exp(activations - normalization_constants)\n      one_minus_t1 = (1.0 - t1)\n      one_minus_t2 = 1.0\n  else:\n    one_minus_t1 = (1.0 - t1)\n    one_minus_t2 = (1.0 - t2)\n    normalization_constants = compute_normalization(\n        activations, t2, num_iters=5)\n    shifted_activations = tf.nn.relu(1.0 + one_minus_t2 *\n                                     (activations - normalization_constants))\n\n  if t1 == 1.0:\n    return tf.reduce_sum(\n        tf.multiply(\n            tf.log(labels + 1e-10) -\n            tf.log(tf.pow(shifted_activations, 1.0 / one_minus_t2)), labels),\n        -1)\n  else:\n    beta = 1.0 + one_minus_t1\n    logt_probs = (tf.pow(shifted_activations, one_minus_t1 / one_minus_t2) -\n                  1.0) / one_minus_t1\n    return tf.reduce_sum(\n        tf.multiply(log_t(labels, t1) - logt_probs, labels) - 1.0 / beta *\n        (tf.pow(labels, beta) -\n         tf.pow(shifted_activations, beta / one_minus_t2)), -1)\n\n\ndef tempered_sigmoid(activations, t, num_iters=5):\n  \"\"\"Tempered sigmoid function.\n  Args:\n    activations: Activations for the positive class for binary classification.\n    t: Temperature tensor > 0.0.\n    num_iters: Number of iterations to run the method.\n  Returns:\n    A probabilities tensor.\n  \"\"\"\n  t = tf.convert_to_tensor(t)\n  input_shape = tf.shape(activations)\n  activations_2d = tf.reshape(activations, [-1, 1])\n  internal_activations = tf.concat(\n      [tf.zeros_like(activations_2d), activations_2d], 1)\n  normalization_constants = tf.cond(\n      # pylint: disable=g-long-lambda\n      tf.equal(t, 1.0),\n      lambda: tf.log(\n          tf.reduce_sum(tf.exp(internal_activations), -1, keep_dims=True)),\n      functools.partial(compute_normalization, internal_activations, t,\n                        num_iters))\n  internal_probabilities = exp_t(internal_activations - normalization_constants,\n                                 t)\n  one_class_probabilities = tf.split(internal_probabilities, 2, axis=1)[1]\n  return tf.reshape(one_class_probabilities, input_shape)\n\n\ndef tempered_softmax(activations, t, num_iters=5):\n  \"\"\"Tempered softmax function.\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature tensor > 0.0.\n    num_iters: Number of iterations to run the method.\n  Returns:\n    A probabilities tensor.\n  \"\"\"\n  t = tf.convert_to_tensor(t)\n  normalization_constants = tf.cond(\n      tf.equal(t, 1.0),\n      lambda: tf.log(tf.reduce_sum(tf.exp(activations), -1, keep_dims=True)),\n      functools.partial(compute_normalization, activations, t, num_iters))\n  return exp_t(activations - normalization_constants, t)\n\n\ndef bi_tempered_binary_logistic_loss(activations,\n                                     labels,\n                                     t1,\n                                     t2,\n                                     label_smoothing=0.0,\n                                     num_iters=5):\n  \"\"\"Bi-Tempered binary logistic loss.\n  Args:\n    activations: A tensor containing activations for class 1.\n    labels: A tensor with shape and dtype as activations.\n    t1: Temperature 1 (< 1.0 for boundedness).\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n    label_smoothing: Label smoothing\n    num_iters: Number of iterations to run the method.\n  Returns:\n    A loss tensor.\n  \"\"\"\n  with tf.name_scope('binary_bitempered_logistic'):\n    t1 = tf.convert_to_tensor(t1)\n    t2 = tf.convert_to_tensor(t2)\n    out_shape = tf.shape(labels)\n    labels_2d = tf.reshape(labels, [-1, 1])\n    activations_2d = tf.reshape(activations, [-1, 1])\n    internal_labels = tf.concat([1.0 - labels_2d, labels_2d], 1)\n    internal_logits = tf.concat([tf.zeros_like(activations_2d), activations_2d],\n                                1)\n    losses = bi_tempered_logistic_loss(internal_logits, internal_labels, t1, t2,\n                                       label_smoothing, num_iters)\n    return tf.reshape(losses, out_shape)\n\n\ndef bi_tempered_logistic_loss(activations,\n                              labels,\n                              t1,\n                              t2,\n                              label_smoothing=0.0,\n                              num_iters=5):\n  \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    labels: A tensor with shape and dtype as activations.\n    t1: Temperature 1 (< 1.0 for boundedness).\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n    label_smoothing: Label smoothing parameter between [0, 1).\n    num_iters: Number of iterations to run the method.\n  Returns:\n    A loss tensor.\n  \"\"\"\n  with tf.name_scope('bitempered_logistic'):\n    t1 = tf.convert_to_tensor(t1)\n    t2 = tf.convert_to_tensor(t2)\n    if label_smoothing > 0.0:\n      num_classes = tf.cast(tf.shape(labels)[-1], tf.float32)\n      labels = (\n          1 - num_classes /\n          (num_classes - 1) * label_smoothing) * labels + label_smoothing / (\n              num_classes - 1)\n\n    @tf.custom_gradient\n    def _custom_gradient_bi_tempered_logistic_loss(activations):\n      \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n      Args:\n        activations: A multi-dimensional tensor with last dim `num_classes`.\n      Returns:\n        A loss tensor, grad.\n      \"\"\"\n      with tf.name_scope('gradient_bitempered_logistic'):\n        probabilities = tempered_softmax(activations, t2, num_iters)\n        loss_values = tf.multiply(\n            labels,\n            log_t(labels + 1e-10, t1) -\n            log_t(probabilities, t1)) - 1.0 / (2.0 - t1) * (\n                tf.pow(labels, 2.0 - t1) - tf.pow(probabilities, 2.0 - t1))\n\n        def grad(d_loss):\n          \"\"\"Explicit gradient calculation.\n          Args:\n            d_loss: Infinitesimal change in the loss value.\n          Returns:\n            Loss gradient.\n          \"\"\"\n          delta_probs = probabilities - labels\n          forget_factor = tf.pow(probabilities, t2 - t1)\n          delta_probs_times_forget_factor = tf.multiply(delta_probs,\n                                                        forget_factor)\n          delta_forget_sum = tf.reduce_sum(\n              delta_probs_times_forget_factor, -1, keep_dims=True)\n          escorts = tf.pow(probabilities, t2)\n          escorts = escorts / tf.reduce_sum(escorts, -1, keep_dims=True)\n          derivative = delta_probs_times_forget_factor - tf.multiply(\n              escorts, delta_forget_sum)\n          return tf.multiply(d_loss, derivative)\n\n        return loss_values, grad\n\n    loss_values = tf.cond(tf.logical_and(tf.equal(t1, 1.0), tf.equal(t2, 1.0)),\n                          functools.partial(\n                              tf.nn.softmax_cross_entropy_with_logits,\n                              labels=labels,\n                              logits=activations),\n                          functools.partial(\n                              _custom_gradient_bi_tempered_logistic_loss,\n                              activations))\n    reduce_sum_last = lambda x: tf.reduce_sum(x, -1)\n    loss_values = tf.cond(tf.logical_and(tf.equal(t1, 1.0), tf.equal(t2, 1.0)),\n                          functools.partial(tf.identity, loss_values),\n                          functools.partial(reduce_sum_last, loss_values))\n    return loss_values\n\n\ndef sparse_bi_tempered_logistic_loss(activations, labels, t1, t2, num_iters=5):\n  \"\"\"Sparse Bi-Tempered Logistic Loss with custom gradient.\n  Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    labels: A tensor with dtype of int32.\n    t1: Temperature 1 (< 1.0 for boundedness).\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n    num_iters: Number of iterations to run the method.\n  Returns:\n    A loss tensor.\n  \"\"\"\n  with tf.name_scope('sparse_bitempered_logistic'):\n    t1 = tf.convert_to_tensor(t1)\n    t2 = tf.convert_to_tensor(t2)\n    num_classes = tf.shape(activations)[-1]\n\n    @tf.custom_gradient\n    def _custom_gradient_sparse_bi_tempered_logistic_loss(activations):\n      \"\"\"Sparse Bi-Tempered Logistic Loss with custom gradient.\n      Args:\n        activations: A multi-dimensional tensor with last dim `num_classes`.\n      Returns:\n        A loss tensor, grad.\n      \"\"\"\n      with tf.name_scope('gradient_sparse_bitempered_logistic'):\n        probabilities = tempered_softmax(activations, t2, num_iters)\n        # TODO(eamid): Replace one hot with gather.\n        loss_values = -log_t(\n            tf.reshape(\n                tf.gather_nd(probabilities,\n                             tf.where(tf.one_hot(labels, num_classes))),\n                tf.shape(activations)[:-1]), t1) - 1.0 / (2.0 - t1) * (\n                    1.0 - tf.reduce_sum(tf.pow(probabilities, 2.0 - t1), -1))\n\n        def grad(d_loss):\n          \"\"\"Explicit gradient calculation.\n          Args:\n            d_loss: Infinitesimal change in the loss value.\n          Returns:\n            Loss gradient.\n          \"\"\"\n          delta_probs = probabilities - tf.one_hot(labels, num_classes)\n          forget_factor = tf.pow(probabilities, t2 - t1)\n          delta_probs_times_forget_factor = tf.multiply(delta_probs,\n                                                        forget_factor)\n          delta_forget_sum = tf.reduce_sum(\n              delta_probs_times_forget_factor, -1, keep_dims=True)\n          escorts = tf.pow(probabilities, t2)\n          escorts = escorts / tf.reduce_sum(escorts, -1, keep_dims=True)\n          derivative = delta_probs_times_forget_factor - tf.multiply(\n              escorts, delta_forget_sum)\n          return tf.multiply(d_loss, derivative)\n\n        return loss_values, grad\n\n    loss_values = tf.cond(\n        tf.logical_and(tf.equal(t1, 1.0), tf.equal(t2, 1.0)),\n        functools.partial(tf.nn.sparse_softmax_cross_entropy_with_logits,\n                          labels=labels, logits=activations),\n        functools.partial(_custom_gradient_sparse_bi_tempered_logistic_loss,\n                          activations))\n    return loss_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compile Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensembled_model.compile(\n    optimizer=tf.keras.optimizers.Adam(lr=0.001), \n    loss='sparse_categorical_crossentropy', \n    metrics=[\"sparse_categorical_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensembled_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_training_dataset(TRAINING_FILENAMES)\nval_dataset =get_validation_dataset(VALID_FILENAMES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\n\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n\nearlystopping = tf.keras.callbacks.EarlyStopping( patience=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Fit model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = ensembled_model.fit(train_dataset,\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    epochs=EPOCHS, \n                    validation_data=val_dataset,\n                    validation_steps=VALID_STEPS,callbacks = [earlystopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensembled_model.save(\"./ensembled_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = get_test_dataset(TEST_FILENAMES)\ntest_images_ds = test_ds.map(lambda image, idnum: image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_predictions = ensembled_model.predict(test_images_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Calculating predictions...')\n\npredictions = np.argmax(ensemble_predictions, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Generating submission file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='image_id,label', comments='')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}